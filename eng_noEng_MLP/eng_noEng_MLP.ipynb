{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from typing import List, Tuple\n",
    "#seed for consistence\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read pickle db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng:List[str] = []\n",
    "no_eng:List[str] = []\n",
    "with (open(r\"eng_no_eng_data.pickle\", \"rb\")) as f:\n",
    "        objects=pickle.load(f)\n",
    "        eng = objects['eng']\n",
    "        no_eng = objects['no_eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching length of both lists\n",
    "no_eng = random.sample(no_eng, len(eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all charecters a-z + ee whaterver to \n",
    "\n",
    "all_letters = sorted(list(set(\"\".join(no_eng + eng))))\n",
    "# print('all_letters: ', all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_embedding(word:str) -> np.ndarray:\n",
    "    \"\"\"takes in a word of 5 characters and encodes it based on all letters array in 5x27 matrix\n",
    "    \"\"\"\n",
    "    all_letters=  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ã±']\n",
    "    word_init:np.ndarray = np.zeros((5,27),dtype=np.float32)\n",
    "\n",
    "    for i,c in enumerate(word):\n",
    "        j = all_letters.index(c)\n",
    "        word_init[i][j]= 1.0\n",
    "    return word_init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_eng_:List[Tuple[np.ndarray,float]] = [(char_embedding(i),0.0) for i in no_eng]\n",
    "eng_:List[Tuple[np.ndarray,float]] = [(char_embedding(i),1.0) for i in eng]\n",
    "random.shuffle(no_eng_)\n",
    "random.shuffle(eng_)\n",
    "\n",
    "# spliting training data, test data and validationg data as 70%,15% and 15% respectively\n",
    "train_split_index = math.floor(0.8 * len(eng_))\n",
    "# test_split_index = math.floor(0.85 * len(eng_))\n",
    "\n",
    "train_set = eng_[:train_split_index]+no_eng_[:train_split_index]\n",
    "test_set = eng_[train_split_index:]+no_eng_[train_split_index:]\n",
    "# validation_set =  eng_[test_split_index:]+ no_eng_[test_split_index:]\n",
    "\n",
    "# total_count = len(no_eng_)+len(eng_)\n",
    "# print(len(train_set),len(test_set),len(validation_set))\n",
    "# print(len(train_set)/total_count,len(test_set)/total_count,len(validation_set)/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64818225\n",
      "Iteration 2, loss = 0.54416578\n",
      "Iteration 3, loss = 0.51697521\n",
      "Iteration 4, loss = 0.50463065\n",
      "Iteration 5, loss = 0.49363071\n",
      "Iteration 6, loss = 0.48454511\n",
      "Iteration 7, loss = 0.47604111\n",
      "Iteration 8, loss = 0.46802400\n",
      "Iteration 9, loss = 0.45905742\n",
      "Iteration 10, loss = 0.45183443\n",
      "Iteration 11, loss = 0.44361471\n",
      "Iteration 12, loss = 0.43721173\n",
      "Iteration 13, loss = 0.43052841\n",
      "Iteration 14, loss = 0.42447812\n",
      "Iteration 15, loss = 0.41865921\n",
      "Iteration 16, loss = 0.41227249\n",
      "Iteration 17, loss = 0.40662452\n",
      "Iteration 18, loss = 0.40198630\n",
      "Iteration 19, loss = 0.39616415\n",
      "Iteration 20, loss = 0.39079842\n",
      "Iteration 21, loss = 0.38712755\n",
      "Iteration 22, loss = 0.38079183\n",
      "Iteration 23, loss = 0.37926158\n",
      "Iteration 24, loss = 0.37250560\n",
      "Iteration 25, loss = 0.36860089\n",
      "Iteration 26, loss = 0.36545692\n",
      "Iteration 27, loss = 0.36142145\n",
      "Iteration 28, loss = 0.35649619\n",
      "Iteration 29, loss = 0.35238637\n",
      "Iteration 30, loss = 0.35040740\n",
      "Iteration 31, loss = 0.34457571\n",
      "Iteration 32, loss = 0.34338086\n",
      "Iteration 33, loss = 0.33847950\n",
      "Iteration 34, loss = 0.33282220\n",
      "Iteration 35, loss = 0.33197153\n",
      "Iteration 36, loss = 0.32867531\n",
      "Iteration 37, loss = 0.32643841\n",
      "Iteration 38, loss = 0.32166157\n",
      "Iteration 39, loss = 0.32235227\n",
      "Iteration 40, loss = 0.31622742\n",
      "Iteration 41, loss = 0.31295461\n",
      "Iteration 42, loss = 0.30846795\n",
      "Iteration 43, loss = 0.30789305\n",
      "Iteration 44, loss = 0.30420047\n",
      "Iteration 45, loss = 0.29980727\n",
      "Iteration 46, loss = 0.29720422\n",
      "Iteration 47, loss = 0.29483066\n",
      "Iteration 48, loss = 0.29383670\n",
      "Iteration 49, loss = 0.28903377\n",
      "Iteration 50, loss = 0.28734835\n",
      "Iteration 51, loss = 0.28406939\n",
      "Iteration 52, loss = 0.28175733\n",
      "Iteration 53, loss = 0.27915308\n",
      "Iteration 54, loss = 0.27812739\n",
      "Iteration 55, loss = 0.27542065\n",
      "Iteration 56, loss = 0.27190547\n",
      "Iteration 57, loss = 0.26969762\n",
      "Iteration 58, loss = 0.26897726\n",
      "Iteration 59, loss = 0.26719297\n",
      "Iteration 60, loss = 0.26179911\n",
      "Iteration 61, loss = 0.26066150\n",
      "Iteration 62, loss = 0.26664186\n",
      "Iteration 63, loss = 0.25537406\n",
      "Iteration 64, loss = 0.25530357\n",
      "Iteration 65, loss = 0.25276169\n",
      "Iteration 66, loss = 0.25233225\n",
      "Iteration 67, loss = 0.24929340\n",
      "Iteration 68, loss = 0.24570824\n",
      "Iteration 69, loss = 0.24644335\n",
      "Iteration 70, loss = 0.24639105\n",
      "Iteration 71, loss = 0.24306900\n",
      "Iteration 72, loss = 0.24016344\n",
      "Iteration 73, loss = 0.23856573\n",
      "Iteration 74, loss = 0.23792157\n",
      "Iteration 75, loss = 0.23586702\n",
      "Iteration 76, loss = 0.23313598\n",
      "Iteration 77, loss = 0.22964290\n",
      "Iteration 78, loss = 0.22745459\n",
      "Iteration 79, loss = 0.22561026\n",
      "Iteration 80, loss = 0.22771960\n",
      "Iteration 81, loss = 0.22398225\n",
      "Iteration 82, loss = 0.22266776\n",
      "Iteration 83, loss = 0.22096265\n",
      "Iteration 84, loss = 0.22029233\n",
      "Iteration 85, loss = 0.21589873\n",
      "Iteration 86, loss = 0.21974668\n",
      "Iteration 87, loss = 0.21367709\n",
      "Iteration 88, loss = 0.21820395\n",
      "Iteration 89, loss = 0.21143081\n",
      "Iteration 90, loss = 0.20690561\n",
      "Iteration 91, loss = 0.20688033\n",
      "Iteration 92, loss = 0.20744540\n",
      "Iteration 93, loss = 0.20626705\n",
      "Iteration 94, loss = 0.20361170\n",
      "Iteration 95, loss = 0.20257460\n",
      "Iteration 96, loss = 0.20400161\n",
      "Iteration 97, loss = 0.20107817\n",
      "Iteration 98, loss = 0.20149070\n",
      "Iteration 99, loss = 0.19489330\n",
      "Iteration 100, loss = 0.19821765\n",
      "Iteration 101, loss = 0.19593229\n",
      "Iteration 102, loss = 0.19535342\n",
      "Iteration 103, loss = 0.19597340\n",
      "Iteration 104, loss = 0.20267547\n",
      "Iteration 105, loss = 0.19065932\n",
      "Iteration 106, loss = 0.18895110\n",
      "Iteration 107, loss = 0.19044421\n",
      "Iteration 108, loss = 0.18624007\n",
      "Iteration 109, loss = 0.18384652\n",
      "Iteration 110, loss = 0.18642000\n",
      "Iteration 111, loss = 0.18750423\n",
      "Iteration 112, loss = 0.18561949\n",
      "Iteration 113, loss = 0.18622425\n",
      "Iteration 114, loss = 0.18604405\n",
      "Iteration 115, loss = 0.18506312\n",
      "Iteration 116, loss = 0.18218812\n",
      "Iteration 117, loss = 0.17869462\n",
      "Iteration 118, loss = 0.17986306\n",
      "Iteration 119, loss = 0.17742760\n",
      "Iteration 120, loss = 0.17515588\n",
      "Iteration 121, loss = 0.17555967\n",
      "Iteration 122, loss = 0.17780079\n",
      "Iteration 123, loss = 0.17802437\n",
      "Iteration 124, loss = 0.17630792\n",
      "Iteration 125, loss = 0.17332885\n",
      "Iteration 126, loss = 0.17460066\n",
      "Iteration 127, loss = 0.17700441\n",
      "Iteration 128, loss = 0.17202058\n",
      "Iteration 129, loss = 0.17178773\n",
      "Iteration 130, loss = 0.17132855\n",
      "Iteration 131, loss = 0.16917627\n",
      "Iteration 132, loss = 0.16864530\n",
      "Iteration 133, loss = 0.16534639\n",
      "Iteration 134, loss = 0.16783627\n",
      "Iteration 135, loss = 0.16854469\n",
      "Iteration 136, loss = 0.16532689\n",
      "Iteration 137, loss = 0.16781323\n",
      "Iteration 138, loss = 0.16383326\n",
      "Iteration 139, loss = 0.16482589\n",
      "Iteration 140, loss = 0.16614282\n",
      "Iteration 141, loss = 0.16722124\n",
      "Iteration 142, loss = 0.16397194\n",
      "Iteration 143, loss = 0.16355454\n",
      "Iteration 144, loss = 0.16713210\n",
      "Iteration 145, loss = 0.16640034\n",
      "Iteration 146, loss = 0.16274633\n",
      "Iteration 147, loss = 0.15855180\n",
      "Iteration 148, loss = 0.16199204\n",
      "Iteration 149, loss = 0.16248952\n",
      "Iteration 150, loss = 0.16148162\n",
      "Iteration 151, loss = 0.16594732\n",
      "Iteration 152, loss = 0.16369813\n",
      "Iteration 153, loss = 0.16030452\n",
      "Iteration 154, loss = 0.15883130\n",
      "Iteration 155, loss = 0.15794791\n",
      "Iteration 156, loss = 0.15550497\n",
      "Iteration 157, loss = 0.15558451\n",
      "Iteration 158, loss = 0.15227446\n",
      "Iteration 159, loss = 0.15377977\n",
      "Iteration 160, loss = 0.15496595\n",
      "Iteration 161, loss = 0.16056630\n",
      "Iteration 162, loss = 0.15369867\n",
      "Iteration 163, loss = 0.15384090\n",
      "Iteration 164, loss = 0.15699868\n",
      "Iteration 165, loss = 0.15209774\n",
      "Iteration 166, loss = 0.15239728\n",
      "Iteration 167, loss = 0.15557225\n",
      "Iteration 168, loss = 0.15277365\n",
      "Iteration 169, loss = 0.14851863\n",
      "Iteration 170, loss = 0.15254820\n",
      "Iteration 171, loss = 0.15562981\n",
      "Iteration 172, loss = 0.15308069\n",
      "Iteration 173, loss = 0.15600860\n",
      "Iteration 174, loss = 0.15475493\n",
      "Iteration 175, loss = 0.15150918\n",
      "Iteration 176, loss = 0.15344276\n",
      "Iteration 177, loss = 0.15967384\n",
      "Iteration 178, loss = 0.15478404\n",
      "Iteration 179, loss = 0.14993677\n",
      "Iteration 180, loss = 0.14870920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.1, hidden_layer_sizes=(128, 4), max_iter=800,\n",
       "              random_state=1, verbose=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [i.flatten() for i,_ in train_set]\n",
    "y = [j for _,j in train_set]\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-1, hidden_layer_sizes=(128, 4), random_state=1,verbose = True,\n",
    "                    max_iter = 800)\n",
    "clf.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945454545454545"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_X = [i.flatten() for i,_ in train_set]\n",
    "eval_y = [j for _,j in train_set]\n",
    "clf.score(eval_X,eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7462476547842402"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_X = [i.flatten() for i,_ in test_set]\n",
    "eval_y = [j for _,j in test_set]\n",
    "clf.score(eval_X,eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n"
     ]
    }
   ],
   "source": [
    "#Ã±\n",
    "sample_word = 'nope'\n",
    "out = clf.predict(char_embedding(sample_word).flatten().reshape(1, -1))\n",
    "if out[0] == 1:\n",
    "    print('english')\n",
    "else:\n",
    "    print(\"not english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
