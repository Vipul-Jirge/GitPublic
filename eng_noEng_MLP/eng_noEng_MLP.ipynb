{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from typing import List, Tuple\n",
    "#seed for consistence\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read pickle db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng:List[str] = []\n",
    "no_eng:List[str] = []\n",
    "with (open(r\"eng_no_eng_data.pickle\", \"rb\")) as f:\n",
    "        objects=pickle.load(f)\n",
    "        eng = objects['eng']\n",
    "        no_eng = objects['no_eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching length of both lists\n",
    "no_eng = random.sample(no_eng, len(eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all charecters a-z + ee whaterver to \n",
    "\n",
    "all_letters = sorted(list(set(\"\".join(no_eng + eng))))\n",
    "# print('all_letters: ', all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_embedding(word:str) -> np.ndarray:\n",
    "    \"\"\"takes in a word of 5 characters and encodes it based on all letters array in 5x27 matrix\n",
    "    \"\"\"\n",
    "    all_letters=  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ã±']\n",
    "    word_init:np.ndarray = np.zeros((5,27),dtype=np.float32)\n",
    "\n",
    "    for i,c in enumerate(word):\n",
    "        j = all_letters.index(c)\n",
    "        word_init[i][j]= 1.0\n",
    "    return word_init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14918 3198 3198\n",
      "0.6999155484657972 0.15004222576710144 0.15004222576710144\n"
     ]
    }
   ],
   "source": [
    "no_eng_:List[Tuple[np.ndarray,float]] = [(char_embedding(i),0.0) for i in no_eng]\n",
    "eng_:List[Tuple[np.ndarray,float]] = [(char_embedding(i),1.0) for i in eng]\n",
    "random.shuffle(no_eng_)\n",
    "random.shuffle(eng_)\n",
    "\n",
    "# spliting training data, test data and validationg data as 70%,15% and 15% respectively\n",
    "train_split_index = math.floor(0.7 * len(eng_))\n",
    "test_split_index = math.floor(0.85 * len(eng_))\n",
    "\n",
    "train_set = eng_[:train_split_index]+no_eng_[:train_split_index]\n",
    "test_set = eng_[train_split_index:test_split_index]+no_eng_[train_split_index:test_split_index]\n",
    "validation_set =  eng_[test_split_index:]+ no_eng_[test_split_index:]\n",
    "\n",
    "total_count = len(no_eng_)+len(eng_)\n",
    "print(len(train_set),len(test_set),len(validation_set))\n",
    "print(len(train_set)/total_count,len(test_set)/total_count,len(validation_set)/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62927874\n",
      "Iteration 2, loss = 0.53542325\n",
      "Iteration 3, loss = 0.49649459\n",
      "Iteration 4, loss = 0.48196836\n",
      "Iteration 5, loss = 0.46915778\n",
      "Iteration 6, loss = 0.45791556\n",
      "Iteration 7, loss = 0.44723071\n",
      "Iteration 8, loss = 0.43637015\n",
      "Iteration 9, loss = 0.42639407\n",
      "Iteration 10, loss = 0.41600728\n",
      "Iteration 11, loss = 0.40620794\n",
      "Iteration 12, loss = 0.39526956\n",
      "Iteration 13, loss = 0.38756276\n",
      "Iteration 14, loss = 0.37749472\n",
      "Iteration 15, loss = 0.36794968\n",
      "Iteration 16, loss = 0.35993653\n",
      "Iteration 17, loss = 0.35270000\n",
      "Iteration 18, loss = 0.34354858\n",
      "Iteration 19, loss = 0.33677668\n",
      "Iteration 20, loss = 0.33014941\n",
      "Iteration 21, loss = 0.32165264\n",
      "Iteration 22, loss = 0.31621035\n",
      "Iteration 23, loss = 0.30978167\n",
      "Iteration 24, loss = 0.30388934\n",
      "Iteration 25, loss = 0.29488462\n",
      "Iteration 26, loss = 0.29058759\n",
      "Iteration 27, loss = 0.28597227\n",
      "Iteration 28, loss = 0.27764890\n",
      "Iteration 29, loss = 0.27313094\n",
      "Iteration 30, loss = 0.26754856\n",
      "Iteration 31, loss = 0.26315755\n",
      "Iteration 32, loss = 0.25695240\n",
      "Iteration 33, loss = 0.25301861\n",
      "Iteration 34, loss = 0.24778907\n",
      "Iteration 35, loss = 0.24101776\n",
      "Iteration 36, loss = 0.23626973\n",
      "Iteration 37, loss = 0.23285658\n",
      "Iteration 38, loss = 0.22926098\n",
      "Iteration 39, loss = 0.22356221\n",
      "Iteration 40, loss = 0.21933313\n",
      "Iteration 41, loss = 0.21217201\n",
      "Iteration 42, loss = 0.20818891\n",
      "Iteration 43, loss = 0.20625278\n",
      "Iteration 44, loss = 0.19847323\n",
      "Iteration 45, loss = 0.19529872\n",
      "Iteration 46, loss = 0.19071373\n",
      "Iteration 47, loss = 0.19059319\n",
      "Iteration 48, loss = 0.18330568\n",
      "Iteration 49, loss = 0.17863781\n",
      "Iteration 50, loss = 0.17651933\n",
      "Iteration 51, loss = 0.17300377\n",
      "Iteration 52, loss = 0.16922759\n",
      "Iteration 53, loss = 0.16508255\n",
      "Iteration 54, loss = 0.16525681\n",
      "Iteration 55, loss = 0.15869969\n",
      "Iteration 56, loss = 0.15369105\n",
      "Iteration 57, loss = 0.15067780\n",
      "Iteration 58, loss = 0.14646792\n",
      "Iteration 59, loss = 0.14399845\n",
      "Iteration 60, loss = 0.13778764\n",
      "Iteration 61, loss = 0.13745997\n",
      "Iteration 62, loss = 0.13269893\n",
      "Iteration 63, loss = 0.13173224\n",
      "Iteration 64, loss = 0.12714641\n",
      "Iteration 65, loss = 0.12765767\n",
      "Iteration 66, loss = 0.12570692\n",
      "Iteration 67, loss = 0.11846246\n",
      "Iteration 68, loss = 0.11504896\n",
      "Iteration 69, loss = 0.11262904\n",
      "Iteration 70, loss = 0.10915700\n",
      "Iteration 71, loss = 0.10690417\n",
      "Iteration 72, loss = 0.10293268\n",
      "Iteration 73, loss = 0.10184394\n",
      "Iteration 74, loss = 0.10032960\n",
      "Iteration 75, loss = 0.09510666\n",
      "Iteration 76, loss = 0.09562343\n",
      "Iteration 77, loss = 0.09188645\n",
      "Iteration 78, loss = 0.08966449\n",
      "Iteration 79, loss = 0.08773906\n",
      "Iteration 80, loss = 0.08356829\n",
      "Iteration 81, loss = 0.08242454\n",
      "Iteration 82, loss = 0.08096717\n",
      "Iteration 83, loss = 0.07789485\n",
      "Iteration 84, loss = 0.07726379\n",
      "Iteration 85, loss = 0.07356834\n",
      "Iteration 86, loss = 0.07119596\n",
      "Iteration 87, loss = 0.06882854\n",
      "Iteration 88, loss = 0.06682028\n",
      "Iteration 89, loss = 0.06675007\n",
      "Iteration 90, loss = 0.06644143\n",
      "Iteration 91, loss = 0.06135980\n",
      "Iteration 92, loss = 0.05764485\n",
      "Iteration 93, loss = 0.05717688\n",
      "Iteration 94, loss = 0.05471696\n",
      "Iteration 95, loss = 0.05633277\n",
      "Iteration 96, loss = 0.05343696\n",
      "Iteration 97, loss = 0.05248526\n",
      "Iteration 98, loss = 0.04881856\n",
      "Iteration 99, loss = 0.04723126\n",
      "Iteration 100, loss = 0.05038152\n",
      "Iteration 101, loss = 0.04511127\n",
      "Iteration 102, loss = 0.04379315\n",
      "Iteration 103, loss = 0.04308868\n",
      "Iteration 104, loss = 0.03905612\n",
      "Iteration 105, loss = 0.03900800\n",
      "Iteration 106, loss = 0.03620048\n",
      "Iteration 107, loss = 0.03606208\n",
      "Iteration 108, loss = 0.03948083\n",
      "Iteration 109, loss = 0.03455186\n",
      "Iteration 110, loss = 0.03378319\n",
      "Iteration 111, loss = 0.03384319\n",
      "Iteration 112, loss = 0.03197783\n",
      "Iteration 113, loss = 0.03044253\n",
      "Iteration 114, loss = 0.03124035\n",
      "Iteration 115, loss = 0.03407685\n",
      "Iteration 116, loss = 0.02830576\n",
      "Iteration 117, loss = 0.02636813\n",
      "Iteration 118, loss = 0.02414627\n",
      "Iteration 119, loss = 0.02314317\n",
      "Iteration 120, loss = 0.02419878\n",
      "Iteration 121, loss = 0.02185690\n",
      "Iteration 122, loss = 0.02288493\n",
      "Iteration 123, loss = 0.02137208\n",
      "Iteration 124, loss = 0.02126460\n",
      "Iteration 125, loss = 0.01983335\n",
      "Iteration 126, loss = 0.02115895\n",
      "Iteration 127, loss = 0.02023946\n",
      "Iteration 128, loss = 0.01892372\n",
      "Iteration 129, loss = 0.01716447\n",
      "Iteration 130, loss = 0.01820093\n",
      "Iteration 131, loss = 0.02034287\n",
      "Iteration 132, loss = 0.01752354\n",
      "Iteration 133, loss = 0.01512120\n",
      "Iteration 134, loss = 0.01510382\n",
      "Iteration 135, loss = 0.01321347\n",
      "Iteration 136, loss = 0.01284454\n",
      "Iteration 137, loss = 0.01300643\n",
      "Iteration 138, loss = 0.01213557\n",
      "Iteration 139, loss = 0.01270687\n",
      "Iteration 140, loss = 0.01146038\n",
      "Iteration 141, loss = 0.01134209\n",
      "Iteration 142, loss = 0.01173280\n",
      "Iteration 143, loss = 0.01044955\n",
      "Iteration 144, loss = 0.01006997\n",
      "Iteration 145, loss = 0.00905305\n",
      "Iteration 146, loss = 0.01022494\n",
      "Iteration 147, loss = 0.01084049\n",
      "Iteration 148, loss = 0.00987769\n",
      "Iteration 149, loss = 0.00966971\n",
      "Iteration 150, loss = 0.01049649\n",
      "Iteration 151, loss = 0.01043006\n",
      "Iteration 152, loss = 0.01492994\n",
      "Iteration 153, loss = 0.05678897\n",
      "Iteration 154, loss = 0.07563804\n",
      "Iteration 155, loss = 0.05039105\n",
      "Iteration 156, loss = 0.02136613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(128, 4), max_iter=800,\n",
       "              random_state=1, verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [i.flatten() for i,_ in train_set]\n",
    "y = [j for _,j in train_set]\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(128, 4), random_state=1,verbose = True,\n",
    "                    warm_start = True, max_iter = 800)\n",
    "clf.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985923045984716"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_X = [i.flatten() for i,_ in train_set]\n",
    "eval_y = [j for _,j in train_set]\n",
    "clf.score(eval_X,eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7439024390243902"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_X = [i.flatten() for i,_ in test_set]\n",
    "eval_y = [j for _,j in test_set]\n",
    "clf.score(eval_X,eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_word = 'vipul'\n",
    "clf.predict(char_embedding(sample_word).flatten().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
